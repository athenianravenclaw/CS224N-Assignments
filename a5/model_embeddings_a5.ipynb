{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dMj1OkS1j4r8"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ModelEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Class that converts input words to their embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, vocab):\n",
        "        \"\"\"\n",
        "        Init the Embedding layers.\n",
        "\n",
        "        @param embed_size (int): Embedding size (dimensionality)\n",
        "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
        "                              See vocab.py for documentation.\n",
        "        \"\"\"\n",
        "        super(ModelEmbeddings, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # default values\n",
        "        self.source = None\n",
        "        self.target = None\n",
        "\n",
        "        src_pad_token_idx = vocab.src['<pad>']\n",
        "        tgt_pad_token_idx = vocab.tgt['<pad>']\n",
        "\n",
        "        ### YOUR CODE HERE (~2 Lines)\n",
        "        ### TODO - Initialize the following variables:\n",
        "        ###     self.source (Embedding Layer for source language)\n",
        "        ###     self.target (Embedding Layer for target langauge)\n",
        "        ###\n",
        "        ### Note:\n",
        "        ###     1. `vocab` object contains two vocabularies:\n",
        "        ###            `vocab.src` for source\n",
        "        ###            `vocab.tgt` for target\n",
        "        ###     2. You can get the length of a specific vocabulary by running:\n",
        "        ###             `len(vocab.<specific_vocabulary>)`\n",
        "        ###     3. Remember to include the padding token for the specific vocabulary\n",
        "        ###        when creating your Embedding.\n",
        "        ###\n",
        "        ### Use the following docs to properly initialize these variables:\n",
        "        ###     Embedding Layer:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
        "        self.source= torch.nn.Embedding(num_embeddings= len(vocab.src) , embedding_dim=embed_size, padding_idx=src_pad_token_idx)\n",
        "        self.source= torch.nn.Embedding(num_embeddings= len(vocab.tgt) , embedding_dim=embed_size, padding_idx=tgt_pad_token_idx)\n",
        "\n",
        "        ### END YOUR CODE\n"
      ]
    }
  ]
}