{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn4wzXxKSfdq",
        "outputId": "afa71159-ac4c-406b-976f-028e42ec6679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import math\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "\n",
        "\n",
        "import sentencepiece as spm\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZqmJU_NIoR0",
        "outputId": "73fe95db-b093-4008-9491-18ce13894807"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.3 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sents(sents, pad_token):\n",
        "    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n",
        "        The paddings should be at the end of each sentence.\n",
        "    @param sents (list[list[str]]): list of sentences, where each sentence\n",
        "                                    is represented as a list of words\n",
        "    @param pad_token (str): padding token\n",
        "    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n",
        "        than the max length sentence are padded out with the pad_token, such that\n",
        "        each sentences in the batch now has equal length.\n",
        "    \"\"\"\n",
        "    sents_padded = []\n",
        "\n",
        "    ### YOUR CODE HERE (~6 Lines)\n",
        "    lengths= [len(sent) for sent in sents]\n",
        "    max_len = max(lengths)\n",
        "    for length, sent in zip(lengths, sents):\n",
        "          sents_padded.append(sent + (max_len-length)*pad_token)\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return sents_padded"
      ],
      "metadata": {
        "id": "5SCMZyY-IuXk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_corpus(file_path, source, vocab_size=2500):\n",
        "    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
        "    @param file_path (str): path to file containing corpus\n",
        "    @param source (str): \"tgt\" or \"src\" indicating whether text\n",
        "        is of the source language or target language\n",
        "    @param vocab_size (int): number of unique subwords in\n",
        "        vocabulary when reading and tokenizing\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.load('{}.model'.format(source))\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            subword_tokens = sp.encode_as_pieces(line)\n",
        "            # only append <s> and </s> to the target sentence\n",
        "            if source == 'tgt':\n",
        "                subword_tokens = ['<s>'] + subword_tokens + ['</s>']\n",
        "            data.append(subword_tokens)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "FJLU_iOfR2cB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autograder_read_corpus(file_path, source):\n",
        "    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
        "    @param file_path (str): path to file containing corpus\n",
        "    @param source (str): \"tgt\" or \"src\" indicating whether text\n",
        "        is of the source language or target language\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for line in open(file_path):\n",
        "        sent = nltk.word_tokenize(line)\n",
        "        # only append <s> and </s> to the target sentence\n",
        "        if source == 'tgt':\n",
        "            sent = ['<s>'] + sent + ['</s>']\n",
        "        data.append(sent)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "OVjBP0AmS2It"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_iter(data, batch_size, shuffle=False):\n",
        "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
        "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
        "    @param batch_size (int): batch size\n",
        "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
        "    \"\"\"\n",
        "    batch_num = math.ceil(len(data) / batch_size)\n",
        "    index_array = list(range(len(data)))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(index_array)\n",
        "\n",
        "    for i in range(batch_num):\n",
        "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
        "        examples = [data[idx] for idx in indices]\n",
        "\n",
        "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
        "        src_sents = [e[0] for e in examples]\n",
        "        tgt_sents = [e[1] for e in examples]\n",
        "\n",
        "        yield src_sents, tgt_sents\n"
      ],
      "metadata": {
        "id": "T8wCicdTT_2L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aH_s13IUZMMw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}