{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NQrlSdA25a_nQ9kksJ78FsrpLTRxVrnv",
      "authorship_tag": "ABX9TyNx6a3jMMORwLlw+r00EmYQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athenianravenclaw/CS244N-Assignments/blob/main/a3(To_debug).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MwjYpzKvcJi2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _minibatch(data, minibatch_idx):\n",
        "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]"
      ],
      "metadata": {
        "id": "F2szUiYxKKDI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minibatches(data, minibatch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Iterates through the provided data one minibatch at at time. You can use this function to\n",
        "    iterate through data in minibatches as follows:\n",
        "\n",
        "        for inputs_minibatch in get_minibatches(inputs, minibatch_size):\n",
        "            ...\n",
        "\n",
        "    Or with multiple data sources:\n",
        "\n",
        "        for inputs_minibatch, labels_minibatch in get_minibatches([inputs, labels], minibatch_size):\n",
        "            ...\n",
        "\n",
        "    Args:\n",
        "        data: there are two possible values:\n",
        "            - a list or numpy array\n",
        "            - a list where each element is either a list or numpy array\n",
        "        minibatch_size: the maximum number of items in a minibatch\n",
        "        shuffle: whether to randomize the order of returned data\n",
        "    Returns:\n",
        "        minibatches: the return value depends on data:\n",
        "            - If data is a list/array it yields the next minibatch of data.\n",
        "            - If data a list of lists/arrays it returns the next minibatch of each element in the\n",
        "              list. This can be used to iterate through multiple data sources\n",
        "              (e.g., features and labels) at the same time.\n",
        "\n",
        "    \"\"\"\n",
        "    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n",
        "    data_size = len(data[0]) if list_data else len(data)\n",
        "    indices = np.arange(data_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
        "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
        "        yield [_minibatch(d, minibatch_indices) for d in data] if list_data \\\n",
        "            else _minibatch(data, minibatch_indices)\n"
      ],
      "metadata": {
        "id": "KDl--EVTE4hx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_all_close(name, actual, expected):\n",
        "    if actual.shape != expected.shape:\n",
        "        raise ValueError(\"{:} failed, expected output to have shape {:} but has shape {:}\"\n",
        "                         .format(name, expected.shape, actual.shape))\n",
        "    if np.amax(np.fabs(actual - expected)) > 1e-6:\n",
        "        raise ValueError(\"{:} failed, expected {:} but value is {:}\".format(name, expected, actual))\n",
        "    else:\n",
        "        print(name, \"passed!\")"
      ],
      "metadata": {
        "id": "1itEE0tTKJLg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import logging\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_uK7XOHaKTkV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P_PREFIX = '<p>:'\n",
        "L_PREFIX = '<l>:'\n",
        "UNK = '<UNK>'\n",
        "NULL = '<NULL>'\n",
        "ROOT = '<ROOT>'"
      ],
      "metadata": {
        "id": "hOrqq11vZKTG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Config(object):\n",
        "    language = 'english'\n",
        "    with_punct = True\n",
        "    unlabeled = True\n",
        "    lowercase = True\n",
        "    use_pos = True\n",
        "    use_dep = True\n",
        "    use_dep = use_dep and (not unlabeled)\n",
        "    data_path = './content'\n",
        "    train_file = '/content/train.conll'\n",
        "    dev_file = '/content/dev.conll'\n",
        "    test_file = '/content/test.conll'\n",
        "    embedding_file = '/content/en-cw.txt'\n"
      ],
      "metadata": {
        "id": "R1O-pzsrZphQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Parser(object):\n",
        "    \"\"\"Contains everything needed for transition-based dependency parsing except for the model\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        root_labels = list([l for ex in dataset\n",
        "                           for (h, l) in zip(ex['head'], ex['label']) if h == 0])\n",
        "        counter = Counter(root_labels)\n",
        "        if len(counter) > 1:\n",
        "            logging.info('Warning: more than one root label')\n",
        "            logging.info(counter)\n",
        "        self.root_label = counter.most_common()[0][0]\n",
        "        deprel = [self.root_label] + list(set([w for ex in dataset\n",
        "                                               for w in ex['label']\n",
        "                                               if w != self.root_label]))\n",
        "        tok2id = {L_PREFIX + l: i for (i, l) in enumerate(deprel)}\n",
        "        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)\n",
        "\n",
        "        config = Config()\n",
        "        self.unlabeled = config.unlabeled\n",
        "        self.with_punct = config.with_punct\n",
        "        self.use_pos = config.use_pos\n",
        "        self.use_dep = config.use_dep\n",
        "        self.language = config.language\n",
        "\n",
        "        if self.unlabeled:\n",
        "            trans = ['L', 'R', 'S']\n",
        "            self.n_deprel = 1\n",
        "        else:\n",
        "            trans = ['L-' + l for l in deprel] + ['R-' + l for l in deprel] + ['S']\n",
        "            self.n_deprel = len(deprel)\n",
        "\n",
        "        self.n_trans = len(trans)\n",
        "        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n",
        "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
        "\n",
        "        # logging.info('Build dictionary for part-of-speech tags.')\n",
        "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)\n",
        "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
        "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
        "\n",
        "        # logging.info('Build dictionary for words.')\n",
        "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[UNK] = self.UNK = len(tok2id)\n",
        "        tok2id[NULL] = self.NULL = len(tok2id)\n",
        "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
        "\n",
        "        self.tok2id = tok2id\n",
        "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
        "\n",
        "        self.n_features = 18 + (18 if config.use_pos else 0) + (12 if config.use_dep else 0)\n",
        "        self.n_tokens = len(tok2id)\n",
        "\n",
        "    def vectorize(self, examples):\n",
        "        vec_examples = []\n",
        "        for ex in examples:\n",
        "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
        "                                  else self.UNK for w in ex['word']]\n",
        "            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
        "                                   else self.P_UNK for w in ex['pos']]\n",
        "            head = [-1] + ex['head']\n",
        "            label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id\n",
        "                            else -1 for w in ex['label']]\n",
        "            vec_examples.append({'word': word, 'pos': pos,\n",
        "                                 'head': head, 'label': label})\n",
        "        return vec_examples\n",
        "\n",
        "    def extract_features(self, stack, buf, arcs, ex):\n",
        "        if stack[0] == \"ROOT\":\n",
        "            stack[0] = 0\n",
        "\n",
        "        def get_lc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
        "\n",
        "        def get_rc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
        "                          reverse=True)\n",
        "\n",
        "        p_features = []\n",
        "        l_features = []\n",
        "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
        "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
        "        if self.use_pos:\n",
        "            p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
        "            p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
        "\n",
        "        for i in range(2):\n",
        "            if i < len(stack):\n",
        "                k = stack[-i-1]\n",
        "                lc = get_lc(k)\n",
        "                rc = get_rc(k)\n",
        "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
        "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
        "\n",
        "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
        "\n",
        "                if self.use_pos:\n",
        "                    p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
        "\n",
        "                if self.use_dep:\n",
        "                    l_features.append(ex['label'][lc[0]] if len(lc) > 0 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][rc[0]] if len(rc) > 0 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][lc[1]] if len(lc) > 1 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][rc[1]] if len(rc) > 1 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][llc[0]] if len(llc) > 0 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][rrc[0]] if len(rrc) > 0 else self.L_NULL)\n",
        "            else:\n",
        "                features += [self.NULL] * 6\n",
        "                if self.use_pos:\n",
        "                    p_features += [self.P_NULL] * 6\n",
        "                if self.use_dep:\n",
        "                    l_features += [self.L_NULL] * 6\n",
        "\n",
        "        features += p_features + l_features\n",
        "        assert len(features) == self.n_features\n",
        "        return features\n",
        "\n",
        "    def get_oracle(self, stack, buf, ex):\n",
        "        if len(stack) < 2:\n",
        "            return self.n_trans - 1\n",
        "\n",
        "        i0 = stack[-1]\n",
        "        i1 = stack[-2]\n",
        "        h0 = ex['head'][i0]\n",
        "        h1 = ex['head'][i1]\n",
        "        l0 = ex['label'][i0]\n",
        "        l1 = ex['label'][i1]\n",
        "\n",
        "        if self.unlabeled:\n",
        "            if (i1 > 0) and (h1 == i0):\n",
        "                return 0\n",
        "            elif (i1 >= 0) and (h0 == i1) and \\\n",
        "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
        "                return 1\n",
        "            else:\n",
        "                return None if len(buf) == 0 else 2\n",
        "        else:\n",
        "            if (i1 > 0) and (h1 == i0):\n",
        "                return l1 if (l1 >= 0) and (l1 < self.n_deprel) else None\n",
        "            elif (i1 >= 0) and (h0 == i1) and \\\n",
        "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
        "                return l0 + self.n_deprel if (l0 >= 0) and (l0 < self.n_deprel) else None\n",
        "            else:\n",
        "                return None if len(buf) == 0 else self.n_trans - 1\n",
        "\n",
        "    def create_instances(self, examples):\n",
        "        all_instances = []\n",
        "        succ = 0\n",
        "        for id, ex in enumerate(examples):\n",
        "            n_words = len(ex['word']) - 1\n",
        "\n",
        "            # arcs = {(h, t, label)}\n",
        "            stack = [0]\n",
        "            buf = [i + 1 for i in range(n_words)]\n",
        "            arcs = []\n",
        "            instances = []\n",
        "            for i in range(n_words * 2):\n",
        "                gold_t = self.get_oracle(stack, buf, ex)\n",
        "                if gold_t is None:\n",
        "                    break\n",
        "                legal_labels = self.legal_labels(stack, buf)\n",
        "                assert legal_labels[gold_t] == 1\n",
        "                instances.append((self.extract_features(stack, buf, arcs, ex),\n",
        "                                  legal_labels, gold_t))\n",
        "                if gold_t == self.n_trans - 1:\n",
        "                    stack.append(buf[0])\n",
        "                    buf = buf[1:]\n",
        "                elif gold_t < self.n_deprel:\n",
        "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
        "                    stack = stack[:-2] + [stack[-1]]\n",
        "                else:\n",
        "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
        "                    stack = stack[:-1]\n",
        "            else:\n",
        "                succ += 1\n",
        "                all_instances += instances\n",
        "\n",
        "        return all_instances\n",
        "\n",
        "    def legal_labels(self, stack, buf):\n",
        "        labels = ([1] if len(stack) > 2 else [0]) * self.n_deprel\n",
        "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel\n",
        "        labels += [1] if len(buf) > 0 else [0]\n",
        "        return labels\n",
        "\n",
        "    def parse(self, dataset, eval_batch_size=5000):\n",
        "        sentences = []\n",
        "        sentence_id_to_idx = {}\n",
        "        for i, example in enumerate(dataset):\n",
        "            n_words = len(example['word']) - 1\n",
        "            sentence = [j + 1 for j in range(n_words)]\n",
        "            sentences.append(sentence)\n",
        "            sentence_id_to_idx[id(sentence)] = i\n",
        "\n",
        "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
        "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
        "\n",
        "        UAS = all_tokens = 0.0\n",
        "        with tqdm(total=len(dataset)) as prog:\n",
        "            for i, ex in enumerate(dataset):\n",
        "                head = [-1] * len(ex['word'])\n",
        "                for h, t, in dependencies[i]:\n",
        "                    head[t] = h\n",
        "                for pred_h, gold_h, gold_l, pos in \\\n",
        "                        zip(head[1:], ex['head'][1:], ex['label'][1:], ex['pos'][1:]):\n",
        "                        assert self.id2tok[pos].startswith(P_PREFIX)\n",
        "                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
        "                        if (self.with_punct) or (not punct(self.language, pos_str)):\n",
        "                            UAS += 1 if pred_h == gold_h else 0\n",
        "                            all_tokens += 1\n",
        "                prog.update(i + 1)\n",
        "        UAS /= all_tokens\n",
        "        return UAS, dependencies\n",
        "\n",
        "\n",
        "class ModelWrapper(object):\n",
        "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
        "        self.parser = parser\n",
        "        self.dataset = dataset\n",
        "        self.sentence_id_to_idx = sentence_id_to_idx\n",
        "\n",
        "    def predict(self, partial_parses):\n",
        "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dependencies,\n",
        "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
        "                for p in partial_parses]\n",
        "        mb_x = np.array(mb_x).astype('int32')\n",
        "        mb_x = torch.from_numpy(mb_x).long()\n",
        "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
        "\n",
        "        pred = self.parser.model(mb_x)\n",
        "        pred = pred.detach().numpy()\n",
        "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
        "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
        "        return pred\n",
        "\n",
        "\n",
        "def read_conll(in_file, lowercase=False, max_example=None):\n",
        "    examples = []\n",
        "    with open(in_file) as f:\n",
        "        word, pos, head, label = [], [], [], []\n",
        "        for line in f.readlines():\n",
        "            sp = line.strip().split('\\t')\n",
        "            if len(sp) == 10:\n",
        "                if '-' not in sp[0]:\n",
        "                    word.append(sp[1].lower() if lowercase else sp[1])\n",
        "                    pos.append(sp[4])\n",
        "                    head.append(int(sp[6]))\n",
        "                    label.append(sp[7])\n",
        "            elif len(word) > 0:\n",
        "                examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
        "                word, pos, head, label = [], [], [], []\n",
        "                if (max_example is not None) and (len(examples) == max_example):\n",
        "                    break\n",
        "        if len(word) > 0:\n",
        "            examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
        "    return examples\n",
        "\n",
        "\n",
        "def build_dict(keys, n_max=None, offset=0):\n",
        "    count = Counter()\n",
        "    for key in keys:\n",
        "        count[key] += 1\n",
        "    ls = count.most_common() if n_max is None \\\n",
        "        else count.most_common(n_max)\n",
        "\n",
        "    return {w[0]: index + offset for (index, w) in enumerate(ls)}\n",
        "\n",
        "\n",
        "def punct(language, pos):\n",
        "    if language == 'english':\n",
        "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
        "    elif language == 'chinese':\n",
        "        return pos == 'PU'\n",
        "    elif language == 'french':\n",
        "        return pos == 'PUNC'\n",
        "    elif language == 'german':\n",
        "        return pos in [\"$.\", \"$,\", \"$[\"]\n",
        "    elif language == 'spanish':\n",
        "        # http://nlp.stanford.edu/software/spanish-faq.shtml\n",
        "        return pos in [\"f0\", \"faa\", \"fat\", \"fc\", \"fd\", \"fe\", \"fg\", \"fh\",\n",
        "                       \"fia\", \"fit\", \"fp\", \"fpa\", \"fpt\", \"fs\", \"ft\",\n",
        "                       \"fx\", \"fz\"]\n",
        "    elif language == 'universal':\n",
        "        return pos == 'PUNCT'\n",
        "    else:\n",
        "        raise ValueError('language: %s is not supported.' % language)\n",
        "\n",
        "\n",
        "def minibatches(data, batch_size):\n",
        "    x = np.array([d[0] for d in data])\n",
        "    y = np.array([d[2] for d in data])\n",
        "    one_hot = np.zeros((y.size, 3))\n",
        "    one_hot[np.arange(y.size), y] = 1\n",
        "    return get_minibatches([x, one_hot], batch_size)\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(reduced=True):\n",
        "    config = Config()\n",
        "\n",
        "    print(\"Loading data...\",)\n",
        "    start = time.time()\n",
        "    train_set = read_conll(os.path.join(config.train_file),\n",
        "                           lowercase=config.lowercase)\n",
        "    dev_set = read_conll(os.path.join(config.dev_file),\n",
        "                         lowercase=config.lowercase)\n",
        "    test_set = read_conll(os.path.join(config.test_file),\n",
        "                          lowercase=config.lowercase)\n",
        "    if reduced:\n",
        "        train_set = train_set[:1000]\n",
        "        dev_set = dev_set[:500]\n",
        "        test_set = test_set[:500]\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print(\"Building parser...\",)\n",
        "    start = time.time()\n",
        "    parser = Parser(train_set)\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print(\"Loading pretrained embeddings...\",)\n",
        "    start = time.time()\n",
        "    word_vectors = {}\n",
        "    for line in open(config.embedding_file).readlines():\n",
        "        sp = line.strip().split()\n",
        "        word_vectors[sp[0]] = [float(x) for x in sp[1:]]\n",
        "    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
        "\n",
        "    for token in parser.tok2id:\n",
        "        i = parser.tok2id[token]\n",
        "        if token in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token]\n",
        "        elif token.lower() in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print(\"Vectorizing data...\",)\n",
        "    start = time.time()\n",
        "    train_set = parser.vectorize(train_set)\n",
        "    dev_set = parser.vectorize(dev_set)\n",
        "    test_set = parser.vectorize(test_set)\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print(\"Preprocessing training data...\",)\n",
        "    start = time.time()\n",
        "    train_examples = parser.create_instances(train_set)\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    return parser, embeddings_matrix, train_examples, dev_set, test_set,\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "7anKUq5N1skh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "Xs_rVa1-hfr9",
        "outputId": "deb957fd-79a5-496e-9307-9fba7ad66f28"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-b17487ed4824>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dev_set = read_conll(os.path.join(config.data_path, config.dev_file),\n\u001b[0m\u001b[1;32m      2\u001b[0m                          lowercase=config.lowercase)\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PartialParse(object):\n",
        "    def __init__(self, sentence):\n",
        "        \"\"\"Initializes this partial parse.\n",
        "\n",
        "        @param sentence (list of str): The sentence to be parsed as a list of words.\n",
        "                                        Your code should not modify the sentence.\n",
        "        \"\"\"\n",
        "        # The sentence being parsed is kept for bookkeeping purposes. Do NOT alter it in your code.\n",
        "        self.sentence = sentence\n",
        "\n",
        "        ### YOUR CODE HERE (3 Lines)\n",
        "        ### Your code should initialize the following fields:\n",
        "        ###     self.stack: The current stack represented as a list with the top of the stack as the\n",
        "        ###                 last element of the list.\n",
        "        ###     self.buffer: The current buffer represented as a list with the first item on the\n",
        "        ###                  buffer as the first item of the list\n",
        "        ###     self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
        "        ###             tuples where each tuple is of the form (head, dependent).\n",
        "        ###             Order for this list doesn't matter.\n",
        "        ###\n",
        "        ### Note: The root token should be represented with the string \"ROOT\"\n",
        "        ### Note: If you need to use the sentence object to initialize anything, make sure to not directly\n",
        "        ###       reference the sentence object.  That is, remember to NOT modify the sentence object.\n",
        "\n",
        "        self.stack = ['ROOT']\n",
        "        self.buffer = self.sentence.copy()\n",
        "        self.dependencies = []\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "\n",
        "    def parse_step(self, transition):\n",
        "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
        "\n",
        "        @param transition (str): A string that equals \"S\", \"LA\", or \"RA\" representing the shift,\n",
        "                                left-arc, and right-arc transitions. You can assume the provided\n",
        "                                transition is a legal transition.\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE (~7-12 Lines)\n",
        "        ### TODO:\n",
        "        ###     Implement a single parsing step, i.e. the logic for the following as\n",
        "        ###     described in the pdf handout:\n",
        "        ###         1. Shift\n",
        "        ###         2. Left Arc\n",
        "        ###         3. Right Arc\n",
        "\n",
        "        if transition == 'S':\n",
        "          if len(self.buffer) >= 0:\n",
        "                 shifted= self.buffer.pop(0)\n",
        "                 self.stack.append(shifted)\n",
        "          else:\n",
        "        # Handle the case when there are not enough elements in the stack\n",
        "        # Print an error message or raise an exception depending on your needs\n",
        "                   print(\"Error: Not enough elements in the stack for 'LA' transition\")\n",
        "\n",
        "\n",
        "        elif transition == 'LA':\n",
        "              if len(self.stack) >= 2:\n",
        "                  self.dependencies.append((self.stack[-1], self.stack.pop(-2)))\n",
        "              else:\n",
        "        # Handle the case when there are not enough elements in the stack\n",
        "        # Print an error message or raise an exception depending on your needs\n",
        "                   print(\"Error: Not enough elements in the stack for 'LA' transition\")\n",
        "\n",
        "\n",
        "        elif transition == 'RA':\n",
        "          if len(self.stack) >= 2:\n",
        "            self.dependencies.append((self.stack[-2],self.stack.pop(-1)))\n",
        "          else:\n",
        "        # Handle the case when there are not enough elements in the stack\n",
        "        # Print an error message or raise an exception depending on your needs\n",
        "                   print(\"Error: Not enough elements in the stack for 'RA' transition\")\n",
        "\n",
        "    def parse(self, transitions):\n",
        "        \"\"\"Applies the provided transitions to this PartialParse\n",
        "\n",
        "        @param transitions (list of str): The list of transitions in the order they should be applied\n",
        "\n",
        "        @return dependencies (list of string tuples): The list of dependencies produced when\n",
        "                                                        parsing the sentence. Represented as a list of\n",
        "                                                        tuples where each tuple is of the form (head, dependent).\n",
        "        \"\"\"\n",
        "        for transition in transitions:\n",
        "            self.parse_step(transition)\n",
        "        return self.dependencies\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-hNtBOiS1h_6"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def minibatch_parse(sentences, model, batch_size):\n",
        "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
        "\n",
        "    @param sentences (list of list of str): A list of sentences to be parsed\n",
        "                                            (each sentence is a list of words and each word is of type string)\n",
        "    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function\n",
        "                                model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
        "                                returns a list of transitions predicted for each parse. That is, after calling\n",
        "                                    transitions = model.predict(partial_parses)\n",
        "                                transitions[i] will be the next transition to apply to partial_parses[i].\n",
        "    @param batch_size (int): The number of PartialParses to include in each minibatch\n",
        "\n",
        "\n",
        "    @return dependencies (list of dependency lists): A list where each element is the dependencies\n",
        "                                                    list for a parsed sentence. Ordering should be the\n",
        "                                                    same as in sentences (i.e., dependencies[i] should\n",
        "                                                    contain the parse for sentences[i]).\n",
        "    \"\"\"\n",
        "    dependencies = []\n",
        "\n",
        "    ### YOUR CODE HERE (~8-10 Lines)\n",
        "    ### TODO:\n",
        "    ###     Implement the minibatch parse algorithm.  Note that the pseudocode for this algorithm is given in the pdf handout.\n",
        "    ###\n",
        "    ###     Note: A shallow copy (as denoted in the PDF) can be made with the \"=\" sign in python, e.g.\n",
        "    ###                 unfinished_parses = partial_parses[:].\n",
        "    ###             Here `unfinished_parses` is a shallow copy of `partial_parses`.\n",
        "    ###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances\n",
        "    ###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.\n",
        "    ###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`\n",
        "    ###             contains references to the same objects. Thus, you should NOT use the `del` operator\n",
        "    ###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that\n",
        "    ###             is being accessed by `partial_parses` and may cause your code to crash.\n",
        "    partial_parses= [PartialParse(sen) for sen in sentences]\n",
        "    unfinished_parses = partial_parses[:]\n",
        "    while len(unfinished_parses)>0:\n",
        "      minibatch_parse= unfinished_parses[:batch_size]                                 #Take the first batch size parses in unfinished parses as a minibatch\n",
        "      minibatch_trans= model.predict(minibatch_parse)                                 #Use the model to predict the next transition for each partial parse in the minibatch\n",
        "      for trans, parses in zip(minibatch_trans, minibatch_parse):\n",
        "        parses.parse_step(trans)                                                      #Perform a parse step on each partial parse in the minibatch with its predicted transition\n",
        "      unfinished_parses=[parses for parses in unfinished_parses\n",
        "                         if not (len(parses.buffer) == 0 and len(parses.stack) == 1)] #Keep if the buffer is not empty and the stack is not of size 1.\n",
        "\n",
        "    for parse1 in partial_parses:\n",
        "      dependencies.append(parse1.dependencies)\n",
        "      return dependencies\n",
        "### END YOUR CODE"
      ],
      "metadata": {
        "id": "IBNnwg2cYkku"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(name, transition, stack, buf, deps,\n",
        "              ex_stack, ex_buf, ex_deps):\n",
        "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
        "    pp = PartialParse([])\n",
        "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
        "\n",
        "    pp.parse_step(transition)\n",
        "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
        "    assert stack == ex_stack, \\\n",
        "        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n",
        "    assert buf == ex_buf, \\\n",
        "        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n",
        "    assert deps == ex_deps, \\\n",
        "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
        "    print(\"{:} test passed!\".format(name))"
      ],
      "metadata": {
        "id": "BMVBV2ZNkOcS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_parse_step():\n",
        "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
        "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
        "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
        "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
        "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
        "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))"
      ],
      "metadata": {
        "id": "5pb7QtfskQ0P"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_parse():\n",
        "    \"\"\"Simple tests for the PartialParse.parse function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
        "    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
        "    dependencies = tuple(sorted(dependencies))\n",
        "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
        "    assert dependencies == expected,  \\\n",
        "        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n",
        "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
        "        \"parse test failed: the input sentence should not be modified\"\n",
        "    print(\"parse test passed!\")"
      ],
      "metadata": {
        "id": "1oSybO-PkYbw"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_parse_step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taUBSRFdl-pn",
        "outputId": "353b2ac8-f5e2-45be-d26e-77c833788fda"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHIFT test passed!\n",
            "LEFT-ARC test passed!\n",
            "RIGHT-ARC test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_parse()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlLUnWJHnQcJ",
        "outputId": "ff606f2e-2598-4bd6-ee8d-fc56ac7e535e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parse test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "V_tyPYlTEwrP"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class ParserModel(nn.Module):\n",
        "    \"\"\" Feedforward neural network with an embedding layer and two hidden layers.\n",
        "    The ParserModel will predict which transition should be applied to a\n",
        "    given partial parse configuration.\n",
        "\n",
        "    PyTorch Notes:\n",
        "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
        "            are a subclass of this \"nn.Module\".\n",
        "        - The \"__init__\" method is where you define all the layers and parameters\n",
        "            (embedding layers, linear layers, dropout layers, etc.).\n",
        "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
        "            when you write \"m = ParserModel()\".\n",
        "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
        "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
        "            in other ParserModel methods.\n",
        "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
        "    \"\"\"\n",
        "    def __init__(self, embeddings, n_features=36,\n",
        "        hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
        "        \"\"\" Initialize the parser model.\n",
        "\n",
        "        @param embeddings (ndarray): word embeddings (num_words, embedding_size)\n",
        "        @param n_features (int): number of input features\n",
        "        @param hidden_size (int): number of hidden units\n",
        "        @param n_classes (int): number of output classes\n",
        "        @param dropout_prob (float): dropout probability\n",
        "        \"\"\"\n",
        "        super(ParserModel, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.embed_size = embeddings.shape[1]\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embeddings = nn.Parameter(torch.tensor(embeddings))\n",
        "\n",
        "        ### YOUR CODE HERE (~9-10 Lines)\n",
        "        ### TODO:\n",
        "        ###     1) Declare `self.embed_to_hidden_weight` and `self.embed_to_hidden_bias` as `nn.Parameter`.\n",
        "        ###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`\n",
        "        ###        with default parameters.\n",
        "        ###     2) Construct `self.dropout` layer.\n",
        "        ###     3) Declare `self.hidden_to_logits_weight` and `self.hidden_to_logits_bias` as `nn.Parameter`.\n",
        "        ###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`\n",
        "        ###        with default parameters.\n",
        "        ###\n",
        "        ### Note: Trainable variables are declared as `nn.Parameter` which is a commonly used API\n",
        "        ###       to include a tensor into a computational graph to support updating w.r.t its gradient.\n",
        "        ###       Here, we use Xavier Uniform Initialization for our Weight initialization.\n",
        "        ###       It has been shown empirically, that this provides better initial weights\n",
        "        ###       for training networks than random uniform initialization.\n",
        "        ###       For more details checkout this great blogpost:\n",
        "        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
        "        ###\n",
        "        ### Please see the following docs for support:\n",
        "        ###     nn.Parameter: https://pytorch.org/docs/stable/nn.html#parameters\n",
        "        ###     Initialization: https://pytorch.org/docs/stable/nn.init.html\n",
        "        ###     Dropout: https://pytorch.org/docs/stable/nn.html#dropout-layers\n",
        "        ###\n",
        "        ### See the PDF for hints.\n",
        "        self.embed_to_hidden_weight= nn.Parameter(torch.empty(self.embed_size*n_features,hidden_size))\n",
        "        self.embed_to_hidden_bias =  nn.Parameter(torch.empty(hidden_size))\n",
        "        self.hidden_to_logits_weight= nn.Parameter(torch.empty(hidden_size, n_classes))\n",
        "        self.hidden_to_logits_bias = nn.Parameter(torch.empty(n_classes))\n",
        "        self.dropout= nn.Dropout(p=dropout_prob)\n",
        "        nn.init.xavier_uniform_(self.embed_to_hidden_weight)\n",
        "        nn.init.xavier_uniform_(self.embed_to_hidden_weight)\n",
        "        nn.init.uniform_(self.embed_to_hidden_weight)\n",
        "        nn.init.uniform_(self.hidden_to_logits_bias)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def embedding_lookup(self, w):\n",
        "        \"\"\" Utilize `w` to select embeddings from embedding matrix `self.embeddings`\n",
        "            @param w (Tensor): input tensor of word indices (batch_size, n_features)\n",
        "\n",
        "            @return x (Tensor): tensor of embeddings for words represented in w\n",
        "                                (batch_size, n_features * embed_size)\n",
        "        \"\"\"\n",
        "\n",
        "        ### YOUR CODE HERE (~1-4 Lines)\n",
        "        ### TODO:\n",
        "        ###     1) For each index `i` in `w`, select `i`th vector from self.embeddings\n",
        "        ###     2) Reshape the tensor using `view` function if necessary\n",
        "        ###\n",
        "        ### Note: All embedding vectors are stacked and stored as a matrix. The model receives\n",
        "        ###       a list of indices representing a sequence of words, then it calls this lookup\n",
        "        ###       function to map indices to sequence of embeddings.\n",
        "        ###\n",
        "        ###       This problem aims to test your understanding of embedding lookup,\n",
        "        ###       so DO NOT use any high level API like nn.Embedding\n",
        "        ###       (we are asking you to implement that!). Pay attention to tensor shapes\n",
        "        ###       and reshape if necessary. Make sure you know each tensor's shape before you run the code!\n",
        "        ###\n",
        "        ### Pytorch has some useful APIs for you, and you can use either one\n",
        "        ### in this problem (except nn.Embedding). These docs might be helpful:\n",
        "        ###     Index select: https://pytorch.org/docs/stable/torch.html#torch.index_select\n",
        "        ###     Gather: https://pytorch.org/docs/stable/torch.html#torch.gather\n",
        "        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
        "        ###     Flatten: https://pytorch.org/docs/stable/generated/torch.flatten.html\n",
        "\n",
        "        y=  torch.index_select(self.embeddings, 0, w.flatten())\n",
        "        x=  y.reshape(w.shape[0],-1)\n",
        "\n",
        "        ### END YOUR CODE\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, w):\n",
        "        \"\"\" Run the model forward.\n",
        "\n",
        "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
        "\n",
        "            PyTorch Notes:\n",
        "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
        "                - When you apply your nn.Module to an input tensor `w` this function is applied to the tensor.\n",
        "                    For example, if you created an instance of your ParserModel and applied it to some `w` as follows,\n",
        "                    the `forward` function would called on `w` and the result would be stored in the `output` variable:\n",
        "                        model = ParserModel()\n",
        "                        output = model(w) # this calls the forward function\n",
        "                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
        "\n",
        "        @param w (Tensor): input tensor of tokens (batch_size, n_features)\n",
        "\n",
        "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
        "                                 without applying softmax (batch_size, n_classes)\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE (~3-5 lines)\n",
        "        ### TODO:\n",
        "        ###     Complete the forward computation as described in write-up. In addition, include a dropout layer\n",
        "        ###     as decleared in `__init__` after ReLU function.\n",
        "        ###\n",
        "        ### Note: We do not apply the softmax to the logits here, because\n",
        "        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
        "        ###\n",
        "        ### Please see the following docs for support:\n",
        "        ###     Matrix product: https://pytorch.org/docs/stable/torch.html#torch.matmul\n",
        "        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n",
        "        x = self.embedding_lookup(w)\n",
        "        h = F.relu_((torch.matmul(x,self.embed_to_hidden_weight))+self.embed_to_hidden_bias)\n",
        "\n",
        "        h = self.dropout(h)\n",
        "        logits = ((torch.matmul(h,self.hidden_to_logits_weight))+self.hidden_to_logits_bias)\n",
        "\n",
        "        ### END YOUR CODE\n",
        "        return logits\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c3hB7SzYnl83"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "import pickle\n",
        "import math\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "from torch import nn, optim\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# Primary Functions\n",
        "# -----------------"
      ],
      "metadata": {
        "id": "b0El44j30pqp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------\n",
        "# Primary Functions\n",
        "# -----------------\n",
        "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
        "    \"\"\" Train the neural dependency parser.\n",
        "\n",
        "    @param parser (Parser): Neural Dependency Parser\n",
        "    @param train_data ():\n",
        "    @param dev_data ():\n",
        "    @param output_path (str): Path to which model weights and results are written.\n",
        "    @param batch_size (int): Number of examples in a single batch\n",
        "    @param n_epochs (int): Number of training epochs\n",
        "    @param lr (float): Learning rate\n",
        "    \"\"\"\n",
        "    best_dev_UAS = 0\n",
        "\n",
        "\n",
        "\n",
        "    ### YOUR CODE HERE (~2-7 lines)\n",
        "    ### TODO:\n",
        "    ###      1) Construct Adam Optimizer in variable `optimizer`\n",
        "    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func` with `mean`\n",
        "    ###         reduction (default)\n",
        "    ###\n",
        "    ### Hint: Use `parser.model.parameters()` to pass optimizer\n",
        "    ###       necessary parameters to tune.\n",
        "    ### Please see the following docs for support:\n",
        "    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n",
        "    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
        "    optimizer= torch.optim.Adam(parser.model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
        "    loss_func= torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
        "        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
        "        if dev_UAS > best_dev_UAS:\n",
        "            best_dev_UAS = dev_UAS\n",
        "            print(\"New best dev UAS! Saving model.\")\n",
        "            torch.save(parser.model.state_dict(), output_path)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
        "    \"\"\" Train the neural dependency parser for single epoch.\n",
        "\n",
        "    Note: In PyTorch we can signify train versus test and automatically have\n",
        "    the Dropout Layer applied and removed, accordingly, by specifying\n",
        "    whether we are training, `model.train()`, or evaluating, `model.eval()`\n",
        "\n",
        "    @param parser (Parser): Neural Dependency Parser\n",
        "    @param train_data ():\n",
        "    @param dev_data ():\n",
        "    @param optimizer (nn.Optimizer): Adam Optimizer\n",
        "    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n",
        "    @param batch_size (int): batch size\n",
        "\n",
        "    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n",
        "    \"\"\"\n",
        "    parser.model.train() # Places model in \"train\" mode, i.e. apply dropout layer\n",
        "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
        "    loss_meter = AverageMeter()\n",
        "\n",
        "    with tqdm(total=(n_minibatches)) as prog:\n",
        "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
        "            optimizer.zero_grad()   # remove any baggage in the optimizer\n",
        "            loss = 0. # store loss for this batch here\n",
        "            train_x = torch.from_numpy(train_x).long()\n",
        "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n",
        "\n",
        "            ### YOUR CODE HERE (~4-10 lines)\n",
        "            ### TODO:\n",
        "            ###      1) Run train_x forward through model to produce `logits`\n",
        "            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n",
        "            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n",
        "            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n",
        "            ###         are the predictions (y^ from the PDF).\n",
        "            ###      3) Backprop losses\n",
        "            ###      4) Take step with the optimizer\n",
        "            ### Please see the following docs for support:\n",
        "            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n",
        "\n",
        "            logits= parser.model.forward(train_x)\n",
        "            loss= loss_func(logits, target=train_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            ### END YOUR CODE\n",
        "            prog.update(1)\n",
        "            loss_meter.update(loss.item())\n",
        "\n",
        "    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n",
        "\n",
        "    print(\"Evaluating on dev set\",)\n",
        "    parser.model.eval() # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
        "    dev_UAS, _ = parser.parse(dev_data)\n",
        "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
        "    return dev_UAS\n"
      ],
      "metadata": {
        "id": "dPYzwauQzcIe"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    assert (torch.__version__.split(\".\") >= [\"1\", \"0\", \"0\"]), \"Please install torch version >= 1.0.0\"\n",
        "\n",
        "    print(80 * \"=\")\n",
        "    print(\"INITIALIZING\")\n",
        "    print(80 * \"=\")\n",
        "    parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data()\n",
        "\n",
        "    start = time.time()\n",
        "    model = ParserModel(embeddings)\n",
        "    parser.model = model\n",
        "    print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
        "\n",
        "    print(80 * \"=\")\n",
        "    print(\"TRAINING\")\n",
        "    print(80 * \"=\")\n",
        "    output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
        "    output_path = output_dir + \"model.weights\"\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n",
        "\n",
        "\n",
        "    print(80 * \"=\")\n",
        "    print(\"TESTING\")\n",
        "    print(80 * \"=\")\n",
        "    print(\"Restoring the best model weights found on the dev set\")\n",
        "    parser.model.load_state_dict(torch.load(output_path))\n",
        "    print(\"Final evaluation on test set\",)\n",
        "    parser.model.eval()\n",
        "    UAS, dependencies = parser.parse(test_data)\n",
        "    print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
        "    print(\"Done!\")"
      ],
      "metadata": {
        "id": "dLHz_ueDBRfo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}